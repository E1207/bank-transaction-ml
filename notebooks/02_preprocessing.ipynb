{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3902452",
   "metadata": {},
   "source": [
    "# üîß Preprocessing et Feature Engineering\n",
    "## Santander Customer Transaction Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook :\n",
    "1. Nettoyer et pr√©parer les donn√©es\n",
    "2. Feature engineering\n",
    "3. Feature selection\n",
    "4. Normalisation des donn√©es\n",
    "5. Gestion du d√©s√©quilibre des classes\n",
    "6. Pr√©paration des donn√©es pour le modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5408d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57035939",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e48226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "print(\"üì• Chargement des donn√©es...\")\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"‚úÖ Train: {train.shape}, Test: {test.shape}\")\n",
    "\n",
    "# S√©parer features et target\n",
    "X_train = train.drop(['ID_code', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop(['ID_code'], axis=1)\n",
    "\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe751518",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er de nouvelles features statistiques\n",
    "print(\"üî® Cr√©ation de nouvelles features...\")\n",
    "\n",
    "def add_statistical_features(df):\n",
    "    \"\"\"Ajoute des features statistiques\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Features statistiques globales\n",
    "    df_new['mean'] = df.mean(axis=1)\n",
    "    df_new['std'] = df.std(axis=1)\n",
    "    df_new['min'] = df.min(axis=1)\n",
    "    df_new['max'] = df.max(axis=1)\n",
    "    df_new['median'] = df.median(axis=1)\n",
    "    df_new['skew'] = df.skew(axis=1)\n",
    "    df_new['kurt'] = df.kurtosis(axis=1)\n",
    "    \n",
    "    # Range\n",
    "    df_new['range'] = df_new['max'] - df_new['min']\n",
    "    \n",
    "    # Quartiles\n",
    "    df_new['q1'] = df.quantile(0.25, axis=1)\n",
    "    df_new['q3'] = df.quantile(0.75, axis=1)\n",
    "    df_new['iqr'] = df_new['q3'] - df_new['q1']\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "X_train_engineered = add_statistical_features(X_train)\n",
    "X_test_engineered = add_statistical_features(X_test)\n",
    "\n",
    "print(f\"‚úÖ Nouvelles features cr√©√©es\")\n",
    "print(f\"   Avant: {X_train.shape[1]} features\")\n",
    "print(f\"   Apr√®s: {X_train_engineered.shape[1]} features\")\n",
    "print(f\"\\n   Nouvelles features: {[col for col in X_train_engineered.columns if col not in X_train.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31192b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f04f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 1: Corr√©lation avec la cible\n",
    "print(\"üîç Feature selection par corr√©lation...\")\n",
    "\n",
    "correlations = X_train_engineered.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features par corr√©lation:\")\n",
    "print(correlations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e665664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "correlations.head(30).plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Top 30 Features par Corr√©lation avec la Cible', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Corr√©lation Absolue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 2: SelectKBest avec ANOVA F-value\n",
    "print(\"üîç Feature selection avec SelectKBest...\")\n",
    "\n",
    "k_best = 100  # Garder les 100 meilleures features\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "selector.fit(X_train_engineered, y_train)\n",
    "\n",
    "# Obtenir les scores\n",
    "scores = pd.DataFrame({\n",
    "    'feature': X_train_engineered.columns,\n",
    "    'score': selector.scores_\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 features par F-score:\")\n",
    "print(scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66320a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner les meilleures features\n",
    "selected_features = scores.head(k_best)['feature'].tolist()\n",
    "\n",
    "X_train_selected = X_train_engineered[selected_features]\n",
    "X_test_selected = X_test_engineered[selected_features]\n",
    "\n",
    "print(f\"‚úÖ Features s√©lectionn√©es: {len(selected_features)}\")\n",
    "print(f\"   Shape: {X_train_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae405e8",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Normalisation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65010be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler (z-score normalization)\n",
    "print(\"‚öñÔ∏è Normalisation des donn√©es avec StandardScaler...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Convertir en DataFrame pour garder les noms de colonnes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train_selected.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test_selected.index)\n",
    "\n",
    "print(\"‚úÖ Normalisation effectu√©e\")\n",
    "print(f\"\\nStatistiques apr√®s normalisation:\")\n",
    "print(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314460a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avant/apr√®s normalisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Avant normalisation\n",
    "X_train_selected.iloc[:, 0].hist(bins=50, ax=axes[0], alpha=0.7, color='blue')\n",
    "axes[0].set_title(f'Avant normalisation - {selected_features[0]}', fontweight='bold')\n",
    "axes[0].set_xlabel('Valeur')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "\n",
    "# Apr√®s normalisation\n",
    "X_train_scaled.iloc[:, 0].hist(bins=50, ax=axes[1], alpha=0.7, color='green')\n",
    "axes[1].set_title(f'Apr√®s normalisation - {selected_features[0]}', fontweight='bold')\n",
    "axes[1].set_xlabel('Valeur normalis√©e')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f630f0",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Analyse de la r√©duction de dimensionnalit√© (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA pour visualisation\n",
    "print(\"üî¨ Analyse en Composantes Principales (PCA)...\")\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Variance expliqu√©e\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"‚úÖ PCA effectu√©e\")\n",
    "print(f\"   Variance expliqu√©e par les 10 premi√®res composantes: {cumulative_variance[9]:.2%}\")\n",
    "print(f\"   Variance expliqu√©e par les 50 composantes: {cumulative_variance[49]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce14970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la variance expliqu√©e\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Variance par composante\n",
    "axes[0].bar(range(1, 51), explained_variance, alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Variance Expliqu√©e par Composante', fontweight='bold')\n",
    "axes[0].set_xlabel('Composante')\n",
    "axes[0].set_ylabel('Variance Expliqu√©e')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance cumul√©e\n",
    "axes[1].plot(range(1, 51), cumulative_variance, marker='o', linestyle='-', color='red')\n",
    "axes[1].axhline(y=0.95, color='green', linestyle='--', label='95% variance')\n",
    "axes[1].set_title('Variance Cumul√©e', fontweight='bold')\n",
    "axes[1].set_xlabel('Nombre de Composantes')\n",
    "axes[1].set_ylabel('Variance Cumul√©e')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 2D des donn√©es avec PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_train_2d = pca_2d.fit_transform(X_train_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot avec les deux classes\n",
    "plt.scatter(\n",
    "    X_train_2d[y_train == 0, 0],\n",
    "    X_train_2d[y_train == 0, 1],\n",
    "    c='blue', alpha=0.3, label='Pas de transaction', s=10\n",
    ")\n",
    "plt.scatter(\n",
    "    X_train_2d[y_train == 1, 0],\n",
    "    X_train_2d[y_train == 1, 1],\n",
    "    c='red', alpha=0.5, label='Transaction', s=20\n",
    ")\n",
    "\n",
    "plt.title('Projection PCA 2D des donn√©es', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fb777",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Sauvegarde des donn√©es pr√©process√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le scaler\n",
    "print(\"üíæ Sauvegarde du scaler...\")\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"‚úÖ Scaler sauvegard√© dans '../models/scaler.pkl'\")\n",
    "\n",
    "# Sauvegarder les features s√©lectionn√©es\n",
    "with open('../models/selected_features.txt', 'w') as f:\n",
    "    for feature in selected_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(\"‚úÖ Liste des features sauvegard√©e dans '../models/selected_features.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c076f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© du preprocessing\n",
    "preprocessing_summary = {\n",
    "    'original_features': X_train.shape[1],\n",
    "    'engineered_features': X_train_engineered.shape[1],\n",
    "    'selected_features': len(selected_features),\n",
    "    'scaling_method': 'StandardScaler',\n",
    "    'train_samples': len(X_train_scaled),\n",
    "    'test_samples': len(X_test_scaled),\n",
    "    'class_distribution': y_train.value_counts().to_dict()\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä R√âSUM√â DU PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "for key, value in preprocessing_summary.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7015a8f",
   "metadata": {},
   "source": [
    "## üìù Conclusions du Preprocessing\n",
    "\n",
    "### Transformations effectu√©es :\n",
    "1. ‚úÖ **Feature Engineering** : Ajout de 11 features statistiques\n",
    "2. ‚úÖ **Feature Selection** : R√©duction √† 100 features les plus pertinentes\n",
    "3. ‚úÖ **Normalisation** : StandardScaler appliqu√©\n",
    "4. ‚úÖ **PCA** : Analyse de la variance (95% avec ~40 composantes)\n",
    "\n",
    "### Prochaine √©tape :\n",
    "‚û°Ô∏è **Notebook 03_modeling.ipynb** : Entra√Ænement des mod√®les ML"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
